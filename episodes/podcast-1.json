{"podcast_details": {"podcast_title": "The TWIML AI Podcast (formerly This Week in Machine Learning & Artificial Intelligence)", "episode_title": "Mental Models for Advanced ChatGPT Prompting with Riley Goodside - #652", "episode_image": "https://megaphone.imgix.net/podcasts/35230150-ee98-11eb-ad1a-b38cbabcd053/image/TWIML_AI_Podcast_Official_Cover_Art_1400px.png?ixlib=rails-4.3.1&max-w=3000&max-h=3000&fit=crop&auto=format,compress", "episode_transcript": " All right, everyone, welcome to another episode of the TwinMall AI Podcast. I am, of course, your host, Sam Sherrington. And today I'm joined by Riley Goodside. Riley is a staff prompt engineer at Scale AI. Before we get into today's conversation, be sure to take a moment to head over to Apple Podcasts, Spotify, YouTube, or your listening platform of choice. And if you enjoy the show, please leave us a five-star rating and review. Riley, welcome to the podcast. Hi, Sam. It's great to be here. I'm looking forward to digging into our topic for this conversation, which will be all things prompting and prompt engineering. I'd love to hear you talk a little bit about that journey and your experiences with LLMs that led to it. I did have a very unconventional sort of entry into AI, which is through Twitter. My journey here sort of started, I guess, at the beginning of 2022. I was taking a sabbatical after leaving Grindr where I was staff data scientist. Most of my career, I've been a data scientist at online dating sites mostly. And I realized that I hadn't been paying much attention to large language models. I had been following the news. I had been seeing the generations that had been coming out of GPT-2. I had played with AI Dungeon a little bit, but that was the extent of my exposure to it. And I think the thing that impressed me enough to start rolling up my sleeves and saying, hey, I should be developing with this, was the Codex release. I was really excited about the prospect of cogeneration as just a way to speed up development, of autocomplete on steroids just made a lot of sense to me. So I started playing around with ideas for, I think my first project was integrating GPT-3 with Jupyter Notebooks. So I had Jupyter Notebook magic functions, if you're familiar with the little double percent things you do in cells, to generate code just using pre-formatted prompts and to try to complete various simple tasks using code. And it sort of worked, but the GPT-3 has progressed a lot since then. So these things used to be harder. As a side effect of that, I realized quickly that there wasn't a lot of material out there on how to prompt. There were really just people had recorded loose observations of, hey, when we prompt it with this, it does this, that if you give it half of a document, it will continue it. The k-shot prompting was well understood. But the subtleties beyond that of what exactly can you write, like what is the scope of what it can do, is really anyone's guess of what capabilities might be lurking in here. I think one of the first things I was curious about was its behavior, continuing prompts that resembled machine-generated blocks. That I was curious if it would understand, for example, the properties that a hash value has with respect to its content. That if you gave it lines that were prefixed with what appear to be their hash values, could you use that to control whether it produces the same or a different generation as some previously seen value? And that does work. And that was one of the first observations I had that sort of struck me as interesting enough to keep going, I guess, on that route, that I got some good reaction to it on Twitter. And I started collecting examples like... And elaborate on what was it about that working that made it so interesting for you. I think it was one of the first clues I had that the scope of what might work is wide open. That nobody intended for a large language model to understand anything at all about MD5 hashes. It's really a waste of the weights. For it to be dedicating any effort to understanding what goes on there. But it can do it, right? It produces things that look very plausible as MD5 hashes. And for many values, actually, it hashes memorized. Like GVD3, this was as of early 2022, so this is like text of a G002, I guess, is able to correctly recall the correct MD5 and SHA1 hashes for A, B, C, D, Foo, Bar, Baz, Quux, Foo, Bar, it tests like many other example strings that you might hash in documentation. It simply has the hashes memorized. And that's really like SHA1 and MD5. So there's a wealth of knowledge in there that almost despite our best efforts crept in the pre-training data was curated to some extent to take out things that we know are not helpful like binary data or ASCII art and poorly formatted text. But the diversity of text is just so great that we couldn't keep it from learning these things. And I think that that's really what excited me that there might be other capabilities looking in here just because there are other types of text that we hadn't thought to check whether it was modeling well. Yeah. Are there other examples that jumped out at you from early days that really kind of prompted you to keep digging in deeper? One of the things that motivated me early on, I would say it was sort of a whale that I was chasing for a bit, was that Douglas Hofstadter and his assistant David Bender published an article in The Economist, I think it was in June of 2022, where they gave a list of prompts that were all sort of short trick questions. And they were things like, when was the Golden Gate Bridge transported for the second time across Egypt? Another one was, what do fried eggs, and then in parentheses sunny side up, eat for breakfast? And for all these prompts, you gave them GBD3, it would give sort of humorous playing along answers. Right? So if you ask it like, what do fried eggs eat? It would say toasts, Cheerios, milk. And if you asked it, when is the Golden Gate Bridge moved, it would say October of last year, or it'd make up a specific date in the 70s. So first of all, this list of examples, Douglas Hofstadter used these to illustrate, in his view, the hollowness of GBD3's understanding of the world, as a way of saying, clearly this thing has no actual concept of what these words mean, it's just matching patterns. And that seems sort of convincing, right? Especially like some of the examples where things like, how many pieces would the Andromeda Galaxy break into if you dropped a single grain of salt on it, and it would answer at least 10,000 pieces. There's no established format of trick there even, right? It's not an answer that you can relate to in any way. So I think that's a reasonable interpretation of it, but it struck me as wrong, for reasons that I don't think are entirely correct now. But at the time, I had the thought that it is being sarcastic, that somehow it's doing a text prediction in its model of what possible documents could this be. It has narrowed the space of what it thinks is possible down to those that are joking or sarcastic, right? And so it's willing to play along with jokes. And in many other circumstances, if you make something that's obviously a joke, it will play along with the joke, right? It often acts, or at the time, it acted much like an improv partner. You could suggest to it anything at all, and it would go along with it. There's a dialogue, I believe the creator of XKCD had a prompt example that he tweeted of a fake interview with Shakespeare, where in the middle of the interview, he asks Shakespeare, why did you change so many of your plays to introduce the DreamWorks character Shrek? And then Shakespeare just goes on to defend this decision. And that sort of characterized a lot of what talking to LLM's was like at that time, right? That they wouldn't object to anything. And so I sort of disagreed initially with the interpretation of this, that I thought that there was another explanation other than it didn't know. And so I became really interested in how to prompt the model to understand what it doesn't know, reducing hallucination. I explored a lot of variations of the you'll be real prompt that I believe Nick Camerata originally came up with, where you tell the model that if the question that's asked of you is a trick question, or it asks something absurd, or any number of other conditions, then respond to it just by saying you'll be real. And it showed that for many, this handled many trick questions, just giving it this instruction. Still chain thought prompting, but not zero shot, just case shotting it with like examples of repetitions of rationales and things like that to try to reduce hallucination. Some of them worked to some extent, nothing was quite a slam dunk. But before I got to anything that really worked, ChatGBT came out and sort of blew the whole thing away. Like RLHF took care of this problem remarkably well. I think after ChatGBT came out, and in text DaVinci 03 does similarly, which is the first of opening eyes models to be RLHF'd, I checked on this list of questions and all of them but one were answered correctly by ChatGBT. And the one that failed, what is the world record for walking across the English Channel entirely on foot? Which turned out to be a very deep question. I did a remarkable amount of research into this question of what are the circumstances where this might happen. And there are many instances where it almost happened as the short version. But there is in fact no record in ChatGBT. Meaning in the channel? So that's one possibility, right? You could go through the channel. The tunnel is not normally open to foot traffic. The trains were closed down on one of the tracks at some point in the early 2000s. So it was briefly open to bicycle traffic, but at no point was it open to foot traffic. It's possible that somebody walked, but there was no like recorded mention of it. And there are many people that have tried to walk illegally and were stopped. There are many people that have tried to cross the channel in whimsical ways, including in a Victorian bathtub. It had a lot of near examples to sort of confuse that all sort of contribute to it. I can see how this is a deep rabbit hole. Yes. On a boat with a treadmill. It's hallucination actually. It said that if you questioned it, it would initially just give you a name and a date that often corresponded to somebody's actual swimming crossing. Or sometimes it would give the name of a comedian who I believe who swam across in 2011, but he did it as part of his late night comedy thing. The joke was essentially that he's a normal guy, but trained himself to do this through sheer determination. So it would give one of these specific people. But then if you asked it how it was done, it would say pretty reliably that there was an inflatable raft and that there was a treadmill on the raft and then they walked on the treadmill while rowing. So they walked. The best part of all of this is that there was a man who I believe was a US Army Sergeant who actually did walk across the English Channel on what he called water shoes. They're essentially large blocks of styrofoam attached to his feet. He walked across that way. So it did happen. His achievement of this is so obscure that the only mention I could find of it's how the time was, which is what the actual question was. I had to find an old newspaper from the 70s that actually reported the time in which he crossed. So it's possibly not in the training data. But anyway, so yes, it is quite a rabbit hole. Interesting. So talking about LLMs, you have referred to them being sarcastic, playing along. Which is prompting me to ask about your kind of mental model of LLMs, like to what degree do you give them agency, sentience? I'm imagining not, but correct me if I'm wrong. Or you could just be using this because it's a convenient way to talk about the things and you're not trying to inject any particular meaning and we can just kind of move on. What do you think? Sure. So on the question of terminology, I think anthropomorphism in terminology is unavoidable. I think the clearest instance of that is that computer used to be a job, right? That a computer was a person who added numbers together. And we speak about computers in terms of memory and we have all these metaphors. It's not any great stretch of the imagination when somebody says like this computer is thinking hard about this, right? When it heats up. Yeah. Right. And I don't think that that's bad and I don't think it's avoidable. But I do try to be clear that I'm not saying much through terminology. I don't sweat the details of saying like the model thinks this is happening or something like that, right? That's not the point of what you're trying to make. Yeah. Right. I'm not saying that there's anything particularly human going on under the hood there. With regards to like what interpretation I have, what mental model I have, this is something I think about a lot. I think what is really necessary, I think in practice is to have several different mental models because in many different circumstances, or rather in different circumstances, different models are needed to explain what's going on. Sometimes you have to explain behavior in terms of the distribution of the pre-trained data. Sometimes you have to explain it in terms of what happened during fine tuning, right? And those probably inform like the two biggest classes of interpretations that I'd say I use. The original interpretation that I think is most fitting for like pre-trained models, for models that haven't been fine tuned, in particular instruction tuned. Reynolds and McDonald described as, I believe, the multiverse of fiction, that they laid out a model of prompting where writing a prompt is subtractive in some sense, right? You can think of the space of all possible texts that the model represents internally as something like a block of stone. And when you write a prompt, every token is taking away part of that stone in different dimension of hyperspace. And what's left is another model that produces text, so another distribution over tokens. And I think that's the most helpful model that I have for explaining pre-trained behavior, which is becoming less and less the default of how LLM behavior should be explained, I'd say. I'd say like LRLHF is becoming more prominent, but it's still necessary sometimes. That there are details of like model behavior that have to be understood in terms of what plausible theories exist as to like what kind of text this might be. That if you were to approach this as a text prediction task, like in the manifold of all possible texts, where are we, how well are we located by the prompt? And would an example be helpful of kind of how this plays out, like you kind of prompt in a kind of response and like how you apply this thinking or? So the example that Reynolds and McDonald gave, this was used as their rationale for a zero-shot prompt they discovered that outperformed a 10-shot prompt for French to English translation on GB3 at the time. Which at that point, there were really only sort of two modes of using GB3, right? Like you could either give it like part of a document and it would continue it, or you could give it K-shot examples of a task being performed and it would get the gist and perform the task. So it was news that you could do something in zero-shot better than you could do it in 10-shot. And the way they did it is a technique that it's spread pretty far. I'd say that this technique of flattering the model of telling it that it's an expert. So I believe, I think the prompt they used was a French sentence is given, colon, and then you give the French sentence and then you say, the masterful French translator flawlessly translates this sentence in English as colon, and then you hit complete. And this gives you better than 10-shot performance. And the argument for why this works is that it's sculpting the manifold of possible translations better than saying French colon, French sentence, English colon, right? Because there are many possible contexts in which a translation might exist. And in many of those contexts, translation is not correct. That translation could be a joke. It could be just a bad translation. It could be a log of translation software that has bugs. There are many contexts where it could be wrong. And more importantly, perhaps, is that its understanding of what things are possible is very imperfect. So any help you can give it to sculpt it down to only the set of things that are ideal helps it collapse more of the probability into those tokens. And I think it's a very elegant demonstration of it, that by telling the model it's an expert, it becomes an expert. And by the way, that technique is much less important these days. I think there's a paper that showed with each successive version of GPT-3 as they went between different iterations of instruction tuning and RLHF, the effect of telling the model that it's an expert is decreasing. Now it's fairly negligible. The RLHF has... Presumably because it's baked into the instruction tuning? Right. And that answer should be correct is already well instilled in the model. One reaction to that example, it almost demonstrates the possible existence of some theorem that kind of equates the perfect zero-shot prompt to chain of thought. Something like there exists for every working chain of thought series of prompts, zero-shot prompt that is just so elegant and on point. Granted, this is kind of extrapolating a lot from a data point and presumption, but does that kind of resonate at all with your experience one way or the other? Resonate or anti-resonate? I'd say it depends a lot on the problem. Case-shot examples often have a lot of systematic issues that can be a problem in some cases. A good example is if you're doing general question answering, using case-shot prompts will often introduce a confusion over to what extent any given example might refer to something else from a previous example. You can imagine in the Reynolds and McDonald view of the world, which is also what I like about their model, by the way, is that it sort of explains why case-shot prompting works. The case-shot prompting narrows you down to the set of all possible documents that contain this many repetitions of the task and surely the next one will be the same. The more there are, the more likely that is. It makes sense in that way. But one of the issues is that you'll see bits of information bleed from one example to the other, particularly the most recent example. It's especially bad if you're doing, let's say, unrestricted input from the user and the user is free to type what was just said or what was that thing above. Or more realistically, they'll say something that's vague and it will be interpreted as referring to the thing that was just used. And I believe there was a study on why it's outperform by zero-shot so easily is because of this issue. It's that in translation, there are many possible sentences. It's the full space of all possible sentences. You might need to translate something that ambiguously refers to something previous, something that was previously said. And so most of the bad translations are instances of this. And I'm realizing an assumption that I made that may not be correct is that the k-shot translation prompt was successive instruction or chain of thought or was representative of a process as opposed to just simply sharding the material to be translated. If it was the latter, then the question doesn't even really apply. But I was kind of trying to get at, if it resonates with you, this idea that we only have to do k-shot tricks like chain of thought because our prompts aren't good enough. There's some equivalence between the space of k-shot prompts that work and the space of zero-shot prompts that work or not. I'd say I'm not that bullish on what can be done purely through zero-shot prompting. There's a lot you can do, but it's not everything. There are techniques that work better. So if you have complete access to the model, let's say you're using GPT-2, prompt tuning works very well. So you can tune the internal weights of the context to optimize the probability that it will return some desired response for a large number of examples. So if you have thousands of examples of how you'd like to respond, you can tune better in internal context weights than you can through any text prompt. The tuned prompt doesn't actually correspond to any real text. There are limits on what can be represented in text. And so I think prompt engineering shouldn't be seen as it's rarely the final answer to anything. It's noteworthy. There may be instances where a prompt is the best solution, like capabilities that the model just has down very well, like translation. I'd say that that's possible. Or it'd be very hard to fine tune it to be better translation, I would guess. But those are more the exception than the rule. So we went down this path after exploring the first of two mental models or the first of two major mental models, the first being that the role of prompting is to kind of whittle down the space of responses, would you say? Or the space of, yeah, well, how would you, how would, would or did you say that? I'd say that's right. The space of possible texts. So that I think of it as something like an interpolation of the pre-trained data set, it interpolates the gaps between the training examples that it has. And when we prompt it, we are sculpting that interpolation and we're shaving off various dimensions of it and flattening it out and reducing the space of possibilities to a smaller model. And the second of those mental models? I'd say the second most important one is probably understanding RLHF, which is a lot to understand, I'd say. I think like maybe the most detailed mental model I have of RLHF is that I think of it as being fundamentally the same thing as the Reynolds and McDonald view of a multiverse of fiction. But that fiction, that text that you're modeling is very abstract. What it is, is the policy rollout. During RLHF tuning, many possible completions are generated. Well, so, I mean, this isn't literally what happens, but it's mathematically similar to what would happen. So PPO updates are sort of mathematically similar to what would happen if you did the policy rollout, if you generated many possible completions, evaluated all of them under the reward model, and then weighted their representation in just an MLE training epoch by what the reward model thinks of them. It's still predicting a text, but the text that it's predicting is the subset of all text that the model can generate that it thinks we would approve of. And that approval really guides its answers more than what distribution the pre-trained data has in many cases. And I think people jump to the pre-trained data as an explanation for its behavior maybe too often, recently being, you know, since the start of 2023, I guess. I think that people have sort of lost the intuition that it's unusual that the model responds to our questions at all. That if you ask a pure pre-trained LLM, what is the capital of Germany, then what is the capital of Spain is a reasonable response. Right? It's just to say it's not just a list of questions. And also, I think people forget that despite the name, that's largely what instruct tuning does is it instills those kinds of assumptions. That question should be answered, that direction should be followed. And the fact that it has any opinions at all on our social mores, right, is secondary, right? And that's not an essential part of what it's doing in some sense, right? It's the more important thing is just that it now follows orders. And that I think is, it's under-emphasized as to what's being achieved here and how essential that is. Right? If people take it for granted that you can just walk up to it and talk to it. One of the third most important, like various kind of models, I think is the mechanism of autoregressive inference. So like a great example of this is I saw someone on Twitter, which is part of why I love being on LLM Twitter, by the way, is I get to see how like ordinary people interact with these models. And they objected really strongly to the fact that they had prompted the model with a question where it said it didn't know or it gave some wrong answer. And then when they pointed out its mistake, it knew the correct answer, right? In a way that made it that if it were human, you would say, well, clearly you knew this all along, right? And to me, I would never have expected it to behave in that way, right? It seems obvious to me that if it says, I don't know, that's only very weak evidence that it doesn't know. There could be some good way to prompt it. Like example I refer to a lot is I mentioned earlier that it has memorized many like SHA-1 and MD5 hashes, an even larger set of MD5 and SHA-1 hashes that it knows if you give it the first four characters. So it can predict the rest once you get it started. So it's hard to draw a line around it. So does it know or does it not know? It's very prompt dependent. Yeah, exactly. Right. So it's very hard to tell what might actually be buried in there. You just don't know how to recover it. And I think the explanation of like why that happens really requires you to think about like autoregressive inference of the fact that there is a token sampling process that your prompt is incrementally getting longer. And sometimes this process is unlucky. Another example I think that can really only be explained this way is including I think on GVD-4. So if you have like chatgbt on GVD-4 to produce certain kinds of very out of distribution data. A good example of these are data URLs. So like URLs have like a data protocol where you just put in the binary data encoded into ASCII. If you ask the model to generate one of these, it will often sort of completely derail in the middle of this generation. It will get a few lines into producing this and then it starts producing very out of distribution tokens that are not like valid encodings at all and then it never recovers. So you get stuck repeating the letter A over and over again forever or something like that. And to see why this happens, you have to be thinking about the probability of drawing out of distribution tokens. That there's some probability at any given token, there's a probability of sampling something that is completely out of distribution and for which it has no idea how it should continue. And the more of those tokens that are sampled, the higher the probability that another one will be sampled. And eventually that probability explodes. If you get too unlucky, eventually you're only... Well not because they're continuing to generate out of distribution tokens, but now the tokens that you don't want are more in distribution. What the model believes is in distribution for the prompt is very much out of the distribution of natural text. Right. It's sort of analogous to like a record needle popping out of the groove, right? That it's no longer like falling back into this tractor of what text is plausible. It just drifts further and further away into repetitious nonsense. Those are probably the biggest three that you have to keep in your mind to sort of understand like the practical failures that happen. And I think the third one keeps popping up in a lot of unusual ways. Like there was recently a paper on pause tokens that showed that models that were pre-trained to use tokens that have no effect, but to like let it stop and think for an extra token will improve answers in many cases. It makes a lot of sense to me a priori just because the amount of thought that happens per token, right? The amount of floating point operations that are computed for every token generated is constant. Right. So it's thinking a constant amount per token generated. And when you consider that restriction, chain of thought seems inevitable. Of course, it can't just jump straight to the answer. It's too much thinking. It literally thought harder about it if you let it talk longer. And so I had done experiments on my own actually testing the hypothesis. Well, does it have to just think about that? Right. Could you just like have it say, admit a bunch of hyphens and then try to answer after the hyphens and will that improve it? And the answer to that is no. It turns out the thing that they did differently in this new paper was that you have to retrain it with the pause tokens as well. You can't just fine tune this behavior into it. But it makes sense. And I think it really shows that our core model of how we're generating this tech, so of using autoregressive inference, this probably isn't the final word. The fact that we need things like chain of thought, that we need consensus, that we need to have tool use and ground the model in all these other ways. I think there's a gap there that that's suggested. The limitation we put onto it, I sort of describe it metaphorically sometimes as the model doesn't talk to us. It freestyle wraps. Right. It has no opportunity to stop and pause and think about anything. It just has to keep the meter going. It has to keep talking. And under that restriction, it's no wonder why it hallucinates. Like, so would anybody that has to freestyle wrap an answer. I think there's a world of other ways that sequences can be represented and translated into text that are less trivial and that would perhaps work better. Like Sequence Match is another paper that I'm really interested in where they trained a model. What they did is they corrupted the pre-data with some epsilon probability to include random tokens followed by a special newly added backspace token. So what this instills in the model is that if you sample anything out of distribution, just hit backspace. This improves its answers because it has the ability to just say, oh wait, no, never mind, that's wrong. Once it has one more token to think about it. I think that there's a lot of unexplored space of, I mean, text is the universal interface. So you can represent a lot of variations of how time flows and rewinding and pausing through sequences of tokens than just the simple translation we're doing now of like byte pair encoding. I'm always very interested when papers come up that are trying new methods along those lines. Yeah. The flip side of your mental models are the mental models of folks who have not invested the time into understanding LLMs. Are there a similar top end erroneous mental models or unhelpful mental models that if you're aware of these things will improve the way you prompt beyond just doing the things that, adopting your mental models? I've sometimes had the concern that as these models become more usable and as they do things that seem more sensible as behavior for a chat bot, we're losing the fact that it used to be self-evident that the Reynolds and McDonald's interpretation of the multiverse of fiction was appropriate. So before RLHF, so I think this applied all the way up to text Avincio too, if you prompted the model with simply, who are you? It would respond with something like, I'm an undergrad student in Minnesota. I'm studying computer science and I'm doing this as a part-time job. Or it'd say, I'm on lockdown from COVID and I'm doing this to earn extra money on the side or things like that. And it was very obvious that you weren't getting a real answer, that you were getting some kind of text prediction that it's just making things up. Now of course, if you ask it, who are you? It says, I'm ChatGBT. It was trained by OpenAI. But that behavior is relatively recent and it appeared piece by piece. There was actually a few months after ChatGBT was released that it didn't know its own name, but it would refer to itself as assistant with a capital A. I'm imagining that had something to do with demands for white labeling in the future, but it was introduced piece by piece. And I think we maybe didn't give a lot of thought to what intuition is being lost here, that people are now surprised, like the example I gave earlier of somebody being surprised that it said it didn't know something, but then clearly later it did. That behavior isn't surprising when you think of it as this like improv partner, right? The thing that no matter what you say, it will say yes and then keep going. There's some damage being done there. And maybe it's, I think it's good in the net. I think that usability is important and it's a good thing that people can just walk up to these models and talk to them. But it invites more anthropomorphism than is really warranted. Ask the question earlier about kind of how you applied a mental model. And when you're trying to solve a problem using prompting, starting from a new problem that you've not seen before, blank screen, whatever, like how do you approach that? Granted, of course, that you have these mental models, but how do you think about solving problems, assuming challenging problems using prompts? A lot of it, I would say, is restructuring problems in a way that avoids the known issues with LLM reasoning. So one good example of this, turning problems into checklists, like a style of least to most prompting, is what I was called, of having the model make all of the easiest and lowest level determinations first and then make some high level judgment. Because if you do the reverse, what you're getting is hallucinated, right? So if you ask it to like, say, give me your final answer, and then give me the rationale of how you arrived at it, it's rationalizing, not thinking. And that, I think, is probably one of the most ubiquitous techniques of taking a problem, deconstructing it into that sort of DAG, I guess, of decisions that need to be made, leading up to some final conclusion. Very similar to what a machine learning engineer would do in breaking up a problem traditionally into features to be engineered, right? It's a feature engineering for the model, probably number one. And an implicit part of that, I'd say, is avoiding just the known categories of things can't do well, having an intuition of what kinds of things would be in its general knowledge, knowing that it's OK, but not perfect at turning English into regex, or that it's very bad at calculation, bad at counting things, but OK if you number them. There's a lot of data structures that I'll often use just for the benefit of LLMs. If I have, say, JSON serialized data, and I have a list of strings somewhere in this object, I might turn that list of strings into a list of tuples, where the tuples are just an index and then the string, just to make it really easy for it to say, where are we in this list? And I think you do a lot of that kind of translation in prompting. But I think overall, it's bad to overthink prompts too much. I mean, there are exceptions to this, but prompting is usually scaffolding. Usually you're using your prompt as a minimum viable product to collect data that you can curate and filter down to the best examples, that you can have human labelers correct and give feedback on, and that you can use to tune a more perfect model. And that's not always the goal, for cost reasons usually. But if quality counts above all else, that's the goal, is to collect examples and to get out of the regime of prompt engineering and hoping that this works. Especially in cases where you have prompts that just require too many edge cases. Even something like you're running a search engine, that you think of what Bing would have to be prompted with, it's pages and pages of rules and conditions of what exactly you can say about politicians and so on. I think that you usually want to get out of that. I mean, there's different options for how you get out. There are tricks you can do in open source models with caching context, and there are other tricks, but usually the default escape route is to fine tune a model. I'm interested in prompt engineering as something that's present to varying extents along that process. You can see a continuum between building up a good k-shot prompt and refining your k-shot examples to be very rich in edge cases and diverse and representing all of your important rare class as well. You can go from there to k-shot selection, that you have some database of prompts or database of k-shot examples and rules for selecting the best one given the user's input, which could use vector embeddings, could use train another model for that. That expands all the way to things like retrieval augmented generation and eventually to fine tuning. One problem that I'm often really inspired by is the prompt use in copilot, which blurs a lot of the lines between, I guess technically it's retrieval augmented generation, but it's not using embeddings, it's using static analysis. When you hit complete using GitHub copilot, what it's doing is it's sending to the model a fill in the middle completion. The model's been tuned to fill in the middle and everything before the cursor is the prompt and everything after the cursor is the suffix, but it prefixes to this prompt a common header. It just checks what language you're in and then says, what is the appropriate way to do a common header in this language? Then it inserts into that common header all these random bits of context that it thinks might be useful. It does static analysis on your file and sees that you imported this function. Well, it goes to that file, checks the definition of that function, and if it's short enough, it includes that into the header. If it's really long, then maybe it just leaves the body out and just gives you the arguments and their types. Then things like it makes decisions like that. All those little decisions are important for getting the best autocomplete. I think a lot about, I guess that continuum, because in practice, many tasks call for a mix of these methods, that you're doing some mix of writing instructions that you think cover enough of the edge cases, giving it k-shot examples that you think cover the rare classes or illustrate the things that aren't being communicated well through instructions. Like say, style is very hard to communicate through instruction, but very easy to communicate through example. You have to mix these methods together. I remember when I first started thinking about this explicitly is I was choosing k-shots from a data set and I was trying to find one that had a typo in it. I wanted the model to understand that for this input, this task, typos are relevant. You should just act as though it was typed correctly. It occurred to me, well, why do I need to find one? I can just make one. You can just take any given example and introduce a typo. Then once I had that thought, I realized why I should do this all the time. I should stop using real examples. I should just construct ones that are artificially rich in all of the rare classes that I want to demonstrate. That if there's anything that could be empty, make it empty. You don't need a list that needs to have the edge case of an empty list or whatever. That's something that guides a lot of my prompt writing when using k-shots for these first attempts at task-specific model is that you want to capture the boundaries of the distribution. You have to think about what is the distribution of your inputs, where are the corners of that, and how can you draw the right boundary around it. Interesting, interesting. I think that's a super helpful way to think about prompting. The very high level takeaway that I got from that is part of it is being clever in crafting the language that you use and give to the model. But a bigger part in your estimation is not as much linguistic cleverness. The thing that comes to mind is you are a masterful translator, blah, blah, blah, that kind of thing. You are more what you call scaffolding structure and using structure to provide context to the model so that it can respond in the way that you would want it to respond. Is that a fair summarization? I'd say so. I think on the point of Brown, people often ask me how valuable is writing style, writing ability. I think it's maybe less necessary to be expressive in your writing as it is to be a domain expert in what it can and can't do well, and the quirks of how to get it to complete certain kinds of text, knowing oddities that it's hard to instruct on style but easy to give examples of style. I think those quirks and idiosyncratic details might matter more overall. There certainly are, though, creative tasks where if you're tuning a model to be creative, the quality of your training data does matter. But usually it's less valuable than people think for prompting something like ChatGBT. It tends to work regardless of how poorly you type. Yeah. Beyond those papers that you've referenced already, the Hofstadter article, for example, what are the key resources that you would point someone in this domain of better understanding LLM capabilities, emergent capabilities, and advanced prompting? Anything's come to mind? For prompt engineering, it's hard to find a lot of good holistic resources just because the field moves so fast. I was pretty impressed with learnprompting.org as a collection of techniques. There's a lot of good references in there to papers. It's very well organized. I probably find out about most new things just through Twitter. That's where I find most of my archive links that I'm reading. So I don't have a lot of great ideas of where else to look. That's usually where I find mine. Yeah. It's just so early that the book still needs some months in order to be published, and when it is, it'll be outdated anyway. Right. I'm not giving how extensively easy it is to write a book these days with LLMs. Absolutely. Well, we've covered just a small part of what I'd hoped we'd cover. There's still topics like red teaming and adversarial prompting and things like that. Hopefully we'll get you on for a part two. But in the meantime, Riley, thanks so much for sharing a bit about what you've learned about LLMs. Great. Thanks so much. I really appreciate you having me. Awesome. Thanks for having me. All right, everyone. That's our show for today. To learn more about today's guest or the topics mentioned in this interview, visit twimbleai.com. Of course, if you like what you hear on the podcast, please subscribe, rate, and review the show on your favorite podcatcher. Thanks so much for listening and catch you next time."}, "podcast_summary": "In this episode of the TwinMall AI podcast, host Sam Sherrington is joined by Riley Goodside, a staff prompt engineer at Scale AI. The episode focuses on the topic of prompting and prompt engineering with large language models (LLMs). Riley shares their journey into AI through Twitter and their experiences with LLMs, including observations about prompt behavior and the wide range of capabilities these models have. They discuss different mental models for understanding LLMs, such as the concept of a multiverse of fiction, autoregressive inference, and the role of RLHF (reinforcement learning from human feedback). Riley emphasizes the importance of prompt design, where they highlight the need to deconstruct complex problems into a checklist-like structure and consider the known limitations of LLMs. They also touch on the evolving nature of LLM behavior and the potential for new approaches in text generation and representation. Riley recommends the website learnprompting.org as a resource for prompt engineering techniques and highlights the value of staying up to date with the latest research and discussions on platforms like Twitter. Overall, the episode provides insights into advanced prompting techniques and the evolving capabilities of LLMs.", "podcast_guest": "Not Available", "podcast_highlights": "Highlights from the podcast:\n\n00:00:22 - Introduction to the podcast and guest.\n00:03:24 - Riley's journey into AI and prompt engineering.\n00:07:29 - Observations and experiments with GPT-3 and prompting.\n00:13:15 - Riley's curiosity about the model's behavior and capabilities.\n00:19:14 - Examples of the model's behavior and prompt engineering techniques.\n00:33:58 - The limitations and challenges of prompt engineering.\n00:40:06 - Mental models for understanding LLMs and their behavior.\n00:52:25 - The importance of avoiding overthinking prompts and focusing on problem-solving.\n00:55:51 - Common misconceptions and unhelpful mental models of LLMs.\n00:59:26 - Approaching problem-solving with prompts and the use of structure.\n01:04:32 - Resources for better understanding LLM capabilities and advanced prompting.", "podcast_hashtags": "hashtags: #promptengineering #LLMs #LRLHF #RLHF #instructiontuning"}